{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "def process_pdfs(pdf_files):\n",
    "    # Create a temporary directory\n",
    "    tmp_dir = tempfile.mkdtemp()\n",
    "    print(\"tmp_dir\", tmp_dir)\n",
    "    # Save uploaded PDFs to the temporary directory\n",
    "    saved_files = []\n",
    "    for pdf_file in pdf_files:\n",
    "        # Construct the destination file path\n",
    "        dest_path = os.path.join(tmp_dir, os.path.basename(pdf_file))\n",
    "        # Move the uploaded file to the temporary directory\n",
    "        shutil.copy(pdf_file, dest_path)\n",
    "        saved_files.append(dest_path)  # Store the path of the saved file\n",
    "\n",
    "    # For demonstration, return the paths of saved files\n",
    "    return saved_files\n",
    "\n",
    "# Create the Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=process_pdfs, \n",
    "    inputs=gr.File(label=\"Upload PDFs\", type=\"filepath\", file_count=\"multiple\"),  # Allow multiple file uploads\n",
    "    outputs=\"text\",\n",
    "    title=\"Multiple PDF Uploader\",\n",
    "    description=\"Upload multiple PDF files to process.\"\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "iface.launch()\n",
    "import gradio as gr\n",
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "def process_pdfs(pdf_files):\n",
    "    # Create a temporary directory\n",
    "    tmp_dir = tempfile.mkdtemp()\n",
    "    print(\"tmp_dir\", tmp_dir)\n",
    "    # Save uploaded PDFs to the temporary directory\n",
    "    saved_files = []\n",
    "    for pdf_file in pdf_files:\n",
    "        # Construct the destination file path\n",
    "        dest_path = os.path.join(tmp_dir, os.path.basename(pdf_file))\n",
    "        # Move the uploaded file to the temporary directory\n",
    "        shutil.copy(pdf_file, dest_path)\n",
    "        saved_files.append(dest_path)  # Store the path of the saved file\n",
    "\n",
    "    # For demonstration, return the paths of saved files\n",
    "    return saved_files\n",
    "\n",
    "# Create the Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=process_pdfs, \n",
    "    inputs=gr.File(label=\"Upload PDFs\", type=\"filepath\", file_count=\"multiple\"),  # Allow multiple file uploads\n",
    "    outputs=\"text\",\n",
    "    title=\"Multiple PDF Uploader\",\n",
    "    description=\"Upload multiple PDF files to process.\"\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install gradio llama_index langchain_community langchain langchain_huggingface llama_index.vector_stores.opensearch llama-index-embeddings-langchain langchain-ollama langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from llama_index.vector_stores.opensearch import OpensearchVectorStore, OpensearchVectorClient\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from opensearchpy import OpenSearch\n",
    "\n",
    "#3\n",
    "    # http endpoint for your cluster (opensearch required for vector index usage)\n",
    "endpoint = \"http://localhost:9200\"\n",
    "    # index to demonstrate the VectorStore impl\n",
    "idx =  \"test_pdf_index\"\n",
    "\n",
    "index_name = \"gradio-brochures-bge-m3-1024\"\n",
    "auth = ('admin', 'LocalIndex@42535') \n",
    "\n",
    "    # Initialize OpenSearch client\n",
    "client = OpenSearch(\n",
    "            hosts=[\"https://localhost:9200/\"],\n",
    "            http_compress=True,\n",
    "            use_ssl=True,  # DONT USE IN PRODUCTION\n",
    "            verify_certs=False,  # DONT USE IN PRODUCTION\n",
    "            ssl_assert_hostname=False,\n",
    "            http_auth=auth,\n",
    "            ssl_show_warn=False,\n",
    "        )\n",
    "\n",
    "# 2. Setup HuggingFace Embeddings using HuggingFaceEmbeddings from LangChain\n",
    "#embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# 3. Connect to an Existing OpenSearch Index with OpenSearchVectorStore\n",
    "#index_name = \"your_existing_index\"\n",
    "\n",
    "model_name = \"BAAI/bge-m3\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "hf = HuggingFaceEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs\n",
    "    )\n",
    "# OpensearchVectorClient stores text in this field by default\n",
    "text_field = \"parsed_pdf_content\"\n",
    "# OpensearchVectorClient stores embeddings in this field by default\n",
    "embedding_field = \"embedding\"\n",
    "\n",
    "# single opensearch index with vector search enabled with hybrid search pipeline\n",
    "client = OpensearchVectorClient(\n",
    "        endpoint=endpoint,\n",
    "        index=index_name,\n",
    "        embedding_field=embedding_field,\n",
    "        text_field=text_field,\n",
    "        #http_auth=auth,\n",
    "        #use_ssl = True,\n",
    "        dim=1024,\n",
    "        #verify_certs = False,\n",
    "        #ssl_assert_hostname = False,\n",
    "        #ssl_show_warn = False,\n",
    "        search_pipeline=\"hybrid-search-pipeline\",\n",
    "        os_client=client\n",
    "        #settings=settings\n",
    "    )\n",
    "vector_store = OpensearchVectorStore(client)\n",
    "\n",
    "#storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "vectorStoreIndex = VectorStoreIndex(storage_context=storage_context, embed_model=hf)\n",
    "\n",
    "# 4. Create the Vector Index\n",
    "#index = VectorIndex(vector_store=vector_store)\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=vectorStoreIndex,\n",
    "    similarity_top_k=10,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 5. Query for Similar Documents\n",
    "query = \"Tell me about london\"\n",
    "\n",
    "# Perform the query\n",
    "response = retriever.query(query)\n",
    "\n",
    "# 6. Output the search results\n",
    "for result in response:\n",
    "    print(f\"Document: {result.text}\\nScore: {result.score}\")\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.cianclarke.com/blog/aws-opensearch-and-langchain/\n",
    "\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from llama_index.vector_stores.opensearch import OpensearchVectorStore, OpensearchVectorClient\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from opensearchpy import OpenSearch\n",
    "# create HuggingFaceEmbeddings with BGE embeddings\n",
    "\n",
    "dimensions = 1024\n",
    "\n",
    "model_name = \"BAAI/bge-m3\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "hf = HuggingFaceEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs\n",
    "    )\n",
    "index_name = \"gradio-brochures-bge-m3-1024\"\n",
    "auth = ('admin', 'LocalIndex@42535') \n",
    "docsearch = OpenSearchVectorSearch(\n",
    "    embedding_function=hf,\n",
    "    opensearch_url=f'https://localhost:9200/',\n",
    "    http_compress=True,\n",
    "    use_ssl=True,  # DONT USE IN PRODUCTION\n",
    "    verify_certs=False,  # DONT USE IN PRODUCTION\n",
    "    ssl_assert_hostname=False,\n",
    "    http_auth=auth,\n",
    "    ssl_show_warn=False,\n",
    "    index_name=index_name\n",
    ")\n",
    "\n",
    "# Document Search\n",
    "query = \"Tell me about London\"\n",
    "\n",
    "docs = docsearch.similarity_search(query, k=2, vector_field=\"embedding\", text_field=\"parsed_pdf_content\")\n",
    "\n",
    "print('Total results:', len(docs))\n",
    "# The result here should be the document which closest resembles our question - the RAG phase actually formats an answer. \n",
    "print('Best result:', docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Structure:\n",
      "[{'id': 1, 'name': 'Apple', 'details_json': '{\"color\": \"Red\", \"price\": 1.99, \"is_fresh\": true}'}, {'id': 2, 'name': 'Banana', 'details_json': '{\"color\": \"Yellow\", \"price\": 0.79, \"origin\": \"Ecuador\"}'}, {'id': 3, 'name': 'Grape', 'details_json': '{\"color\": \"Purple\", \"price\": 2.49, \"attributes\": [\"seedless\", \"sweet\"]}'}]\n",
      "Type of data_list: <class 'list'>\n",
      "Type of the embedded JSON string: <class 'str'>\n",
      "\n",
      "Accessing Embedded JSON Data:\n",
      "Parsed details for the first item: {'color': 'Red', 'price': 1.99, 'is_fresh': True}\n",
      "Type after json.loads(): <class 'dict'>\n",
      "Color of the first item: Red\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# This is the original data structure\n",
    "data_list = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"name\": \"Apple\",\n",
    "        \"details_json\": '{\"color\": \"Red\", \"price\": 1.99, \"is_fresh\": true}'\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"name\": \"Banana\",\n",
    "        \"details_json\": '{\"color\": \"Yellow\", \"price\": 0.79, \"origin\": \"Ecuador\"}'\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"name\": \"Grape\",\n",
    "        \"details_json\": '{\"color\": \"Purple\", \"price\": 2.49, \"attributes\": [\"seedless\", \"sweet\"]}'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Original Data Structure:\")\n",
    "print(data_list)\n",
    "print(f\"Type of data_list: {type(data_list)}\")\n",
    "print(f\"Type of the embedded JSON string: {type(data_list[0]['details_json'])}\")\n",
    "\n",
    "# To access the data within the embedded JSON, you need to parse it\n",
    "# For example, accessing the 'color' of the first item\n",
    "first_item_details_string = data_list[0]['details_json']\n",
    "first_item_details_dict = json.loads(first_item_details_string)\n",
    "\n",
    "print(\"\\nAccessing Embedded JSON Data:\")\n",
    "print(f\"Parsed details for the first item: {first_item_details_dict}\")\n",
    "print(f\"Type after json.loads(): {type(first_item_details_dict)}\")\n",
    "print(f\"Color of the first item: {first_item_details_dict['color']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Structure:\n",
      "[{'rule': 'fnd test', 'sql': 'BAT.FIND_TEST_IND = TRUE', 'category': 'FIND_TEST_IND', 'bat_col_meta_info': \"### FIND_TEST_IND\\n**Attribute Name**: 'FIND_TEST_IND' \\n**Data Types**: BOOLEAN\\n**Description**: A bool field that indicates whether or not an account was part of Find Test. If 'Null', then the account was not part of a FT. \"}]\n",
      "Type of data_list: <class 'list'>\n",
      "Type of the embedded JSON string: <class 'str'>\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# To access the data within the embedded JSON, you need to parse it\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# For example, accessing the 'color' of the first item\u001b[39;00m\n\u001b[32m     20\u001b[39m first_item_details_string = data_list[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mbat_col_meta_info\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m first_item_details_dict = \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst_item_details_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAccessing Embedded JSON Data:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mParsed details for the first item: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfirst_item_details_dict\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.11/Frameworks/Python.framework/Versions/3.13/lib/python3.13/json/__init__.py:352\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    347\u001b[39m     s = s.decode(detect_encoding(s), \u001b[33m'\u001b[39m\u001b[33msurrogatepass\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    349\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    350\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    351\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m352\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    354\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONDecoder\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.11/Frameworks/Python.framework/Versions/3.13/lib/python3.13/json/decoder.py:345\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w=WHITESPACE.match):\n\u001b[32m    341\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    342\u001b[39m \u001b[33;03m    containing a JSON document).\u001b[39;00m\n\u001b[32m    343\u001b[39m \n\u001b[32m    344\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m     obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    346\u001b[39m     end = _w(s, end).end()\n\u001b[32m    347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m end != \u001b[38;5;28mlen\u001b[39m(s):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.11/Frameworks/Python.framework/Versions/3.13/lib/python3.13/json/decoder.py:363\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    361\u001b[39m     obj, end = \u001b[38;5;28mself\u001b[39m.scan_once(s, idx)\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# This is the original data structure\n",
    "data_list = [\n",
    "    {\n",
    "        \"rule\": \"fnd test\",\n",
    "        \"sql\": \"BAT.FIND_TEST_IND = TRUE\",\n",
    "        \"category\": \"FIND_TEST_IND\",\n",
    "        \"bat_col_meta_info\": \"### FIND_TEST_IND\\n**Attribute Name**: 'FIND_TEST_IND' \\n**Data Types**: BOOLEAN\\n**Description**: A bool field that indicates whether or not an account was part of Find Test. If 'Null', then the account was not part of a FT. \"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Original Data Structure:\")\n",
    "print(data_list)\n",
    "print(f\"Type of data_list: {type(data_list)}\")\n",
    "print(f\"Type of the embedded JSON string: {type(data_list[0]['bat_col_meta_info'])}\")\n",
    "\n",
    "# To access the data within the embedded JSON, you need to parse it\n",
    "# For example, accessing the 'color' of the first item\n",
    "first_item_details_string = data_list[0]['bat_col_meta_info']\n",
    "first_item_details_dict = json.loads(first_item_details_string)\n",
    "\n",
    "print(\"\\nAccessing Embedded JSON Data:\")\n",
    "print(f\"Parsed details for the first item: {first_item_details_dict}\")\n",
    "print(f\"Type after json.loads(): {type(first_item_details_dict)}\")\n",
    "print(f\"Color of the first item: {first_item_details_dict['FIND_TEST_IND']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "ORIGINAL DATA LIST:\n",
      "====================================================================================================\n",
      "\n",
      "--- Item 0 ---\n",
      "  rule: 'fnd test'...\n",
      "  sql: 'BAT.FIND_TEST_IND = TRUE'...\n",
      "  category: 'FIND_TEST_IND'...\n",
      "  bat_col_meta_info: \"### FIND_TEST_IND\\n**Attribute Name**: 'FIND_TEST_IND' \\n**Data Types**: BOOLEAN\\n**Description**: ...\n",
      "\n",
      "--- Item 1 ---\n",
      "  id: 123...\n",
      "  name: 'Test User'...\n",
      "  active: True...\n",
      "  score: 95.5...\n",
      "\n",
      "--- Item 2 ---\n",
      "  timestamp: '2024-01-15T10:30:00'...\n",
      "  data: '{\"nested\": \"json\", \"value\": 42}'...\n",
      "  tags: ['tag1', 'tag2']...\n",
      "  metadata: {'key': 'value'}...\n",
      "\n",
      "--- Item 3 ---\n",
      "  decimal_val: '123.456'...\n",
      "  bool_str: 'true'...\n",
      "  null_str: 'null'...\n",
      "  empty: ''...\n",
      "  complex: '### Header\\n**Field**: Value'...\n",
      "\n",
      "====================================================================================================\n",
      "VALIDATION AND CONVERSION ANALYSIS:\n",
      "====================================================================================================\n",
      "\n",
      "--- Item 0 ---\n",
      "\n",
      "  Field: 'rule'\n",
      "    Original Type: str\n",
      "    JSON Native: True\n",
      "    Needs Conversion: False\n",
      "    Detected Format: plain_text\n",
      "\n",
      "  Field: 'sql'\n",
      "    Original Type: str\n",
      "    JSON Native: True\n",
      "    Needs Conversion: False\n",
      "    Detected Format: plain_text\n",
      "\n",
      "  Field: 'category'\n",
      "    Original Type: str\n",
      "    JSON Native: True\n",
      "    Needs Conversion: False\n",
      "    Detected Format: plain_text\n",
      "\n",
      "  Field: 'bat_col_meta_info'\n",
      "    Original Type: str\n",
      "    JSON Native: True\n",
      "    Needs Conversion: True\n",
      "    Detected Format: markdown\n",
      "\n",
      "--- Item 1 ---\n",
      "\n",
      "  Field: 'id'\n",
      "    Original Type: int\n",
      "    JSON Native: True\n",
      "    Needs Conversion: False\n",
      "\n",
      "  Field: 'name'\n",
      "    Original Type: str\n",
      "    JSON Native: True\n",
      "    Needs Conversion: False\n",
      "    Detected Format: plain_text\n",
      "\n",
      "  Field: 'active'\n",
      "    Original Type: bool\n",
      "    JSON Native: True\n",
      "    Needs Conversion: False\n",
      "\n",
      "  Field: 'score'\n",
      "    Original Type: float\n",
      "    JSON Native: True\n",
      "    Needs Conversion: False\n",
      "\n",
      "--- Item 2 ---\n",
      "\n",
      "  Field: 'timestamp'\n",
      "    Original Type: str\n",
      "    JSON Native: True\n",
      "    Needs Conversion: False\n",
      "    Detected Format: iso_datetime\n",
      "\n",
      "  Field: 'data'\n",
      "    Original Type: str\n",
      "    JSON Native: True\n",
      "    Needs Conversion: True\n",
      "    Detected Format: embedded_json\n",
      "\n",
      "  Field: 'tags'\n",
      "    Original Type: list\n",
      "    JSON Native: True\n",
      "    Needs Conversion: False\n",
      "\n",
      "  Field: 'metadata'\n",
      "    Original Type: dict\n",
      "    JSON Native: True\n",
      "    Needs Conversion: False\n",
      "\n",
      "--- Item 3 ---\n",
      "\n",
      "  Field: 'decimal_val'\n",
      "    Original Type: str\n",
      "    JSON Native: True\n",
      "    Needs Conversion: True\n",
      "    Detected Format: numeric_string\n",
      "\n",
      "  Field: 'bool_str'\n",
      "    Original Type: str\n",
      "    JSON Native: True\n",
      "    Needs Conversion: True\n",
      "    Detected Format: boolean_string\n",
      "\n",
      "  Field: 'null_str'\n",
      "    Original Type: str\n",
      "    JSON Native: True\n",
      "    Needs Conversion: False\n",
      "    Detected Format: null_string\n",
      "\n",
      "  Field: 'empty'\n",
      "    Original Type: str\n",
      "    JSON Native: True\n",
      "    Needs Conversion: False\n",
      "    Detected Format: empty_string\n",
      "\n",
      "  Field: 'complex'\n",
      "    Original Type: str\n",
      "    JSON Native: True\n",
      "    Needs Conversion: True\n",
      "    Detected Format: markdown\n",
      "\n",
      "====================================================================================================\n",
      "GENERATING VALID JSON:\n",
      "====================================================================================================\n",
      "[\n",
      "  {\n",
      "    \"rule\": \"fnd test\",\n",
      "    \"sql\": \"BAT.FIND_TEST_IND = TRUE\",\n",
      "    \"category\": \"FIND_TEST_IND\",\n",
      "    \"bat_col_meta_info\": {\n",
      "      \"_format\": \"parsed_markdown\",\n",
      "      \"header\": \"FIND_TEST_IND\",\n",
      "      \"attribute_name\": \"FIND_TEST_IND\",\n",
      "      \"data_types\": \"BOOLEAN\",\n",
      "      \"description\": \"A bool field that indicates whether or not an account was part of Find Test. If 'Null', then the account was not part of a FT.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"id\": 123,\n",
      "    \"name\": \"Test User\",\n",
      "    \"active\": true,\n",
      "    \"score\": 95.5\n",
      "  },\n",
      "  {\n",
      "    \"timestamp\": \"2024-01-15T10:30:00\",\n",
      "    \"data\": {\n",
      "      \"nested\": \"json\",\n",
      "      \"value\": 42\n",
      "    },\n",
      "    \"tags\": [\n",
      "      \"tag1\",\n",
      "      \"tag2\"\n",
      "    ],\n",
      "    \"metadata\": {\n",
      "      \"key\": \"value\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"decimal_val\": \"123.456\",\n",
      "    \"bool_str\": \"true\",\n",
      "    \"null_str\": \"null\",\n",
      "    \"empty\": \"\",\n",
      "    \"complex\": {\n",
      "      \"_format\": \"parsed_markdown\",\n",
      "      \"header\": \"Header\",\n",
      "      \"field\": \"Value\"\n",
      "    }\n",
      "  }\n",
      "]\n",
      "\n",
      "====================================================================================================\n",
      "VALIDATION REPORT:\n",
      "====================================================================================================\n",
      "\n",
      "Total Items: 4\n",
      "Total Fields: 17\n",
      "Invalid Fields Found: 5\n",
      "Conversions Performed: 5\n",
      "Errors: 0\n",
      "\n",
      "--- Invalid Fields Details ---\n",
      "  Item 0, Field 'bat_col_meta_info':\n",
      "    Type: str\n",
      "    Format: markdown\n",
      "  Item 2, Field 'data':\n",
      "    Type: str\n",
      "    Format: embedded_json\n",
      "  Item 3, Field 'decimal_val':\n",
      "    Type: str\n",
      "    Format: numeric_string\n",
      "  Item 3, Field 'bool_str':\n",
      "    Type: str\n",
      "    Format: boolean_string\n",
      "  Item 3, Field 'complex':\n",
      "    Type: str\n",
      "    Format: markdown\n",
      "\n",
      "--- Conversions Performed ---\n",
      "  Item 0, Field 'bat_col_meta_info':\n",
      "    Conversion: parsed_markdown_to_dict\n",
      "    str -> dict\n",
      "  Item 2, Field 'data':\n",
      "    Conversion: parsed_embedded_json\n",
      "    str -> dict\n",
      "  Item 3, Field 'decimal_val':\n",
      "    Conversion: kept_as_numeric_string\n",
      "    str -> str\n",
      "  Item 3, Field 'bool_str':\n",
      "    Conversion: kept_as_boolean_string\n",
      "    str -> str\n",
      "  Item 3, Field 'complex':\n",
      "    Conversion: parsed_markdown_to_dict\n",
      "    str -> dict\n",
      "\n",
      "====================================================================================================\n",
      "VERIFICATION:\n",
      "====================================================================================================\n",
      "✓ Valid JSON generated successfully!\n",
      "✓ Successfully parsed back from JSON\n",
      "✓ Number of items: 4\n",
      "✓ All items are valid JSON objects\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from typing import Any, Dict, List, Union, Tuple\n",
    "from datetime import datetime, date\n",
    "from decimal import Decimal\n",
    "\n",
    "# Sample data_list with various problematic types\n",
    "data_list = [\n",
    "    {\n",
    "        \"rule\": \"fnd test\",\n",
    "        \"sql\": \"BAT.FIND_TEST_IND = TRUE\",\n",
    "        \"category\": \"FIND_TEST_IND\",\n",
    "        \"bat_col_meta_info\": \"### FIND_TEST_IND\\n**Attribute Name**: 'FIND_TEST_IND' \\n**Data Types**: BOOLEAN\\n**Description**: A bool field that indicates whether or not an account was part of Find Test. If 'Null', then the account was not part of a FT. \"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 123,\n",
    "        \"name\": \"Test User\",\n",
    "        \"active\": True,\n",
    "        \"score\": 95.5\n",
    "    },\n",
    "    {\n",
    "        \"timestamp\": \"2024-01-15T10:30:00\",\n",
    "        \"data\": '{\"nested\": \"json\", \"value\": 42}',\n",
    "        \"tags\": [\"tag1\", \"tag2\"],\n",
    "        \"metadata\": {\"key\": \"value\"}\n",
    "    },\n",
    "    {\n",
    "        \"decimal_val\": \"123.456\",\n",
    "        \"bool_str\": \"true\",\n",
    "        \"null_str\": \"null\",\n",
    "        \"empty\": \"\",\n",
    "        \"complex\": \"### Header\\n**Field**: Value\"\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "class JSONValidator:\n",
    "    \"\"\"\n",
    "    A comprehensive JSON validator and converter for Dict[str, Any] structures.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.validation_report = {\n",
    "            \"total_items\": 0,\n",
    "            \"total_fields\": 0,\n",
    "            \"invalid_fields\": [],\n",
    "            \"conversions\": [],\n",
    "            \"errors\": []\n",
    "        }\n",
    "    \n",
    "    def identify_value_type(self, value: Any) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Identify the type and characteristics of a value.\n",
    "        \"\"\"\n",
    "        result = {\n",
    "            \"original_value\": value,\n",
    "            \"original_type\": type(value).__name__,\n",
    "            \"is_json_native\": False,\n",
    "            \"needs_conversion\": False,\n",
    "            \"detected_format\": None,\n",
    "            \"error\": None\n",
    "        }\n",
    "        \n",
    "        # Check if it's a JSON native type (str, int, float, bool, None, list, dict)\n",
    "        json_native_types = (str, int, float, bool, type(None), list, dict)\n",
    "        \n",
    "        if isinstance(value, json_native_types):\n",
    "            result[\"is_json_native\"] = True\n",
    "            \n",
    "            # Even if native type, check if it needs special handling\n",
    "            if isinstance(value, str):\n",
    "                result[\"detected_format\"] = self._classify_string(value)\n",
    "                if result[\"detected_format\"] in [\"embedded_json\", \"markdown\", \"numeric_string\", \"boolean_string\"]:\n",
    "                    result[\"needs_conversion\"] = True\n",
    "            elif isinstance(value, (list, dict)):\n",
    "                # Check nested structures\n",
    "                result[\"needs_conversion\"] = self._needs_nested_conversion(value)\n",
    "        else:\n",
    "            result[\"is_json_native\"] = False\n",
    "            result[\"needs_conversion\"] = True\n",
    "            result[\"detected_format\"] = \"non_json_type\"\n",
    "        \n",
    "        # Test JSON serializability\n",
    "        try:\n",
    "            json.dumps(value)\n",
    "        except (TypeError, ValueError, OverflowError) as e:\n",
    "            result[\"needs_conversion\"] = True\n",
    "            result[\"error\"] = str(e)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _classify_string(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Classify the content type of a string.\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"non_string\"\n",
    "        \n",
    "        text_stripped = text.strip()\n",
    "        \n",
    "        if not text_stripped:\n",
    "            return \"empty_string\"\n",
    "        \n",
    "        # Check for embedded JSON\n",
    "        if text_stripped.startswith(('{', '[')):\n",
    "            try:\n",
    "                json.loads(text_stripped)\n",
    "                return \"embedded_json\"\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Check for markdown\n",
    "        if re.search(r'###\\s+\\w+|\\*\\*.+?\\*\\*', text):\n",
    "            return \"markdown\"\n",
    "        \n",
    "        # Check for XML\n",
    "        if text_stripped.startswith('<') and text_stripped.endswith('>'):\n",
    "            return \"xml\"\n",
    "        \n",
    "        # Check for ISO datetime\n",
    "        iso_date_pattern = r'\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}'\n",
    "        if re.match(iso_date_pattern, text_stripped):\n",
    "            return \"iso_datetime\"\n",
    "        \n",
    "        # Check for date\n",
    "        date_patterns = [\n",
    "            r'^\\d{4}-\\d{2}-\\d{2}$',\n",
    "            r'^\\d{2}/\\d{2}/\\d{4}$',\n",
    "            r'^\\d{2}-\\d{2}-\\d{4}$'\n",
    "        ]\n",
    "        for pattern in date_patterns:\n",
    "            if re.match(pattern, text_stripped):\n",
    "                return \"date_string\"\n",
    "        \n",
    "        # Check for numeric string\n",
    "        try:\n",
    "            float(text_stripped)\n",
    "            return \"numeric_string\"\n",
    "        except ValueError:\n",
    "            pass\n",
    "        \n",
    "        # Check for boolean string\n",
    "        if text_stripped.lower() in ['true', 'false', 'yes', 'no', '1', '0']:\n",
    "            return \"boolean_string\"\n",
    "        \n",
    "        # Check for null string\n",
    "        if text_stripped.lower() in ['null', 'none', 'nil']:\n",
    "            return \"null_string\"\n",
    "        \n",
    "        return \"plain_text\"\n",
    "    \n",
    "    def _needs_nested_conversion(self, value: Union[list, dict]) -> bool:\n",
    "        \"\"\"\n",
    "        Check if nested structures need conversion.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if isinstance(value, dict):\n",
    "                for v in value.values():\n",
    "                    info = self.identify_value_type(v)\n",
    "                    if info[\"needs_conversion\"]:\n",
    "                        return True\n",
    "            elif isinstance(value, list):\n",
    "                for item in value:\n",
    "                    info = self.identify_value_type(item)\n",
    "                    if info[\"needs_conversion\"]:\n",
    "                        return True\n",
    "            return False\n",
    "        except:\n",
    "            return True\n",
    "    \n",
    "    def parse_markdown_to_dict(self, markdown_text: str) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Parse markdown formatted text into a structured dictionary.\n",
    "        \"\"\"\n",
    "        result = {\"_format\": \"parsed_markdown\"}\n",
    "        \n",
    "        # Extract header\n",
    "        header_match = re.search(r'###\\s+(.+?)(?=\\n|$)', markdown_text)\n",
    "        if header_match:\n",
    "            result['header'] = header_match.group(1).strip()\n",
    "        \n",
    "        # Extract key-value pairs with bold keys\n",
    "        pattern = r'\\*\\*(.+?)\\*\\*:\\s*(.+?)(?=\\n\\*\\*|\\n###|$)'\n",
    "        matches = re.findall(pattern, markdown_text, re.DOTALL)\n",
    "        \n",
    "        for key, value in matches:\n",
    "            clean_key = key.strip().lower().replace(' ', '_')\n",
    "            clean_value = value.strip().strip(\"'\\\"\")\n",
    "            result[clean_key] = clean_value\n",
    "        \n",
    "        # If no structured data found, return original\n",
    "        if len(result) == 1:  # Only has _format key\n",
    "            result['raw_content'] = markdown_text\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def convert_value(self, value: Any, type_info: Dict[str, Any]) -> Tuple[Any, str]:\n",
    "        \"\"\"\n",
    "        Convert a value to JSON-compatible format.\n",
    "        Returns: (converted_value, conversion_description)\n",
    "        \"\"\"\n",
    "        if not type_info[\"needs_conversion\"]:\n",
    "            return value, \"no_conversion_needed\"\n",
    "        \n",
    "        original_type = type_info[\"original_type\"]\n",
    "        detected_format = type_info[\"detected_format\"]\n",
    "        \n",
    "        try:\n",
    "            # Handle non-JSON native types\n",
    "            if not type_info[\"is_json_native\"]:\n",
    "                if isinstance(value, (datetime, date)):\n",
    "                    return value.isoformat(), f\"datetime_to_iso_string\"\n",
    "                elif isinstance(value, Decimal):\n",
    "                    return float(value), f\"decimal_to_float\"\n",
    "                elif isinstance(value, bytes):\n",
    "                    return value.decode('utf-8'), f\"bytes_to_string\"\n",
    "                elif isinstance(value, set):\n",
    "                    return list(value), f\"set_to_list\"\n",
    "                elif isinstance(value, tuple):\n",
    "                    return list(value), f\"tuple_to_list\"\n",
    "                elif hasattr(value, '__dict__'):\n",
    "                    return vars(value), f\"object_to_dict\"\n",
    "                else:\n",
    "                    return str(value), f\"custom_type_to_string\"\n",
    "            \n",
    "            # Handle string formats\n",
    "            if isinstance(value, str):\n",
    "                if detected_format == \"embedded_json\":\n",
    "                    try:\n",
    "                        parsed = json.loads(value)\n",
    "                        return parsed, \"parsed_embedded_json\"\n",
    "                    except:\n",
    "                        return value, \"embedded_json_parse_failed\"\n",
    "                \n",
    "                elif detected_format == \"markdown\":\n",
    "                    parsed = self.parse_markdown_to_dict(value)\n",
    "                    return parsed, \"parsed_markdown_to_dict\"\n",
    "                \n",
    "                elif detected_format == \"numeric_string\":\n",
    "                    # Keep as string for JSON, but note it could be converted\n",
    "                    return value, \"kept_as_numeric_string\"\n",
    "                \n",
    "                elif detected_format == \"boolean_string\":\n",
    "                    # Keep as string for JSON, but note it could be converted\n",
    "                    return value, \"kept_as_boolean_string\"\n",
    "                \n",
    "                elif detected_format == \"null_string\":\n",
    "                    return None, \"null_string_to_null\"\n",
    "            \n",
    "            # Handle nested structures\n",
    "            if isinstance(value, dict):\n",
    "                converted = {}\n",
    "                for k, v in value.items():\n",
    "                    v_info = self.identify_value_type(v)\n",
    "                    converted[k], _ = self.convert_value(v, v_info)\n",
    "                return converted, \"converted_nested_dict\"\n",
    "            \n",
    "            elif isinstance(value, list):\n",
    "                converted = []\n",
    "                for item in value:\n",
    "                    item_info = self.identify_value_type(item)\n",
    "                    converted_item, _ = self.convert_value(item, item_info)\n",
    "                    converted.append(converted_item)\n",
    "                return converted, \"converted_nested_list\"\n",
    "            \n",
    "            return value, \"no_conversion_applied\"\n",
    "        \n",
    "        except Exception as e:\n",
    "            # If conversion fails, return string representation\n",
    "            return str(value), f\"conversion_error_fallback: {str(e)}\"\n",
    "    \n",
    "    def validate_and_convert_dict(self, data_dict: Dict[str, Any], item_index: int) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Validate and convert a single dictionary.\n",
    "        \"\"\"\n",
    "        converted_dict = {}\n",
    "        \n",
    "        for key, value in data_dict.items():\n",
    "            self.validation_report[\"total_fields\"] += 1\n",
    "            \n",
    "            # Identify value type\n",
    "            type_info = self.identify_value_type(value)\n",
    "            \n",
    "            # Convert if needed\n",
    "            if type_info[\"needs_conversion\"]:\n",
    "                converted_value, conversion_desc = self.convert_value(value, type_info)\n",
    "                converted_dict[key] = converted_value\n",
    "                \n",
    "                # Log the conversion\n",
    "                self.validation_report[\"invalid_fields\"].append({\n",
    "                    \"item_index\": item_index,\n",
    "                    \"field\": key,\n",
    "                    \"original_type\": type_info[\"original_type\"],\n",
    "                    \"detected_format\": type_info[\"detected_format\"],\n",
    "                    \"error\": type_info[\"error\"]\n",
    "                })\n",
    "                \n",
    "                self.validation_report[\"conversions\"].append({\n",
    "                    \"item_index\": item_index,\n",
    "                    \"field\": key,\n",
    "                    \"conversion\": conversion_desc,\n",
    "                    \"from_type\": type(value).__name__,\n",
    "                    \"to_type\": type(converted_value).__name__\n",
    "                })\n",
    "            else:\n",
    "                converted_dict[key] = value\n",
    "        \n",
    "        return converted_dict\n",
    "    \n",
    "    def validate_and_convert_list(self, data_list: List[Dict[str, Any]]) -> Tuple[List[Dict[str, Any]], Dict]:\n",
    "        \"\"\"\n",
    "        Validate and convert entire list of dictionaries.\n",
    "        Returns: (converted_list, validation_report)\n",
    "        \"\"\"\n",
    "        self.validation_report[\"total_items\"] = len(data_list)\n",
    "        converted_list = []\n",
    "        \n",
    "        for idx, item in enumerate(data_list):\n",
    "            if not isinstance(item, dict):\n",
    "                self.validation_report[\"errors\"].append({\n",
    "                    \"item_index\": idx,\n",
    "                    \"error\": f\"Item is not a dictionary, it's a {type(item).__name__}\"\n",
    "                })\n",
    "                # Try to convert to dict or skip\n",
    "                converted_list.append({\"error\": f\"Invalid item type: {type(item).__name__}\", \"original\": str(item)})\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                converted_dict = self.validate_and_convert_dict(item, idx)\n",
    "                converted_list.append(converted_dict)\n",
    "            except Exception as e:\n",
    "                self.validation_report[\"errors\"].append({\n",
    "                    \"item_index\": idx,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "                converted_list.append({\"error\": str(e), \"original_keys\": list(item.keys())})\n",
    "        \n",
    "        return converted_list, self.validation_report\n",
    "    \n",
    "    def generate_json(self, data_list: List[Dict[str, Any]], indent: int = 2) -> str:\n",
    "        \"\"\"\n",
    "        Main method: Validate, convert, and generate valid JSON.\n",
    "        \"\"\"\n",
    "        # Convert the data\n",
    "        converted_list, report = self.validate_and_convert_list(data_list)\n",
    "        \n",
    "        # Generate JSON\n",
    "        try:\n",
    "            json_output = json.dumps(converted_list, indent=indent, ensure_ascii=False)\n",
    "            \n",
    "            # Validate by parsing back\n",
    "            json.loads(json_output)\n",
    "            \n",
    "            return json_output\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to generate valid JSON: {str(e)}\")\n",
    "\n",
    "\n",
    "# Usage\n",
    "def main():\n",
    "    print(\"=\" * 100)\n",
    "    print(\"ORIGINAL DATA LIST:\")\n",
    "    print(\"=\" * 100)\n",
    "    for idx, item in enumerate(data_list):\n",
    "        print(f\"\\n--- Item {idx} ---\")\n",
    "        for key, value in item.items():\n",
    "            print(f\"  {key}: {repr(value)[:100]}...\")\n",
    "    \n",
    "    # Create validator\n",
    "    validator = JSONValidator()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"VALIDATION AND CONVERSION ANALYSIS:\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    # Process each item\n",
    "    for idx, item in enumerate(data_list):\n",
    "        print(f\"\\n--- Item {idx} ---\")\n",
    "        for key, value in item.items():\n",
    "            type_info = validator.identify_value_type(value)\n",
    "            print(f\"\\n  Field: '{key}'\")\n",
    "            print(f\"    Original Type: {type_info['original_type']}\")\n",
    "            print(f\"    JSON Native: {type_info['is_json_native']}\")\n",
    "            print(f\"    Needs Conversion: {type_info['needs_conversion']}\")\n",
    "            if type_info['detected_format']:\n",
    "                print(f\"    Detected Format: {type_info['detected_format']}\")\n",
    "            if type_info['error']:\n",
    "                print(f\"    Error: {type_info['error']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"GENERATING VALID JSON:\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    try:\n",
    "        json_output = validator.generate_json(data_list)\n",
    "        print(json_output)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(\"VALIDATION REPORT:\")\n",
    "        print(\"=\" * 100)\n",
    "        report = validator.validation_report\n",
    "        \n",
    "        print(f\"\\nTotal Items: {report['total_items']}\")\n",
    "        print(f\"Total Fields: {report['total_fields']}\")\n",
    "        print(f\"Invalid Fields Found: {len(report['invalid_fields'])}\")\n",
    "        print(f\"Conversions Performed: {len(report['conversions'])}\")\n",
    "        print(f\"Errors: {len(report['errors'])}\")\n",
    "        \n",
    "        if report['invalid_fields']:\n",
    "            print(\"\\n--- Invalid Fields Details ---\")\n",
    "            for field_info in report['invalid_fields']:\n",
    "                print(f\"  Item {field_info['item_index']}, Field '{field_info['field']}':\")\n",
    "                print(f\"    Type: {field_info['original_type']}\")\n",
    "                print(f\"    Format: {field_info['detected_format']}\")\n",
    "                if field_info['error']:\n",
    "                    print(f\"    Error: {field_info['error']}\")\n",
    "        \n",
    "        if report['conversions']:\n",
    "            print(\"\\n--- Conversions Performed ---\")\n",
    "            for conv in report['conversions']:\n",
    "                print(f\"  Item {conv['item_index']}, Field '{conv['field']}':\")\n",
    "                print(f\"    Conversion: {conv['conversion']}\")\n",
    "                print(f\"    {conv['from_type']} -> {conv['to_type']}\")\n",
    "        \n",
    "        if report['errors']:\n",
    "            print(\"\\n--- Errors ---\")\n",
    "            for error in report['errors']:\n",
    "                print(f\"  Item {error['item_index']}: {error['error']}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(\"VERIFICATION:\")\n",
    "        print(\"=\" * 100)\n",
    "        \n",
    "        # Verify JSON is valid\n",
    "        parsed_back = json.loads(json_output)\n",
    "        print(f\"✓ Valid JSON generated successfully!\")\n",
    "        print(f\"✓ Successfully parsed back from JSON\")\n",
    "        print(f\"✓ Number of items: {len(parsed_back)}\")\n",
    "        print(f\"✓ All items are valid JSON objects\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error generating JSON: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "OPTION 1: Formatted Text (Human Readable)\n",
      "================================================================================\n",
      "{\n",
      "  \"items\": [\n",
      "    {\n",
      "      \"chunkTokens\": \"rule: fnd test\\nsql: BAT.FIND_TEST_IND = TRUE\\ncategory: FIND_TEST_IND\\nbat_col_meta_info:\\n  ### FIND_TEST_IND\\n  **Attribute Name**: 'FIND_TEST_IND' \\n  **Data Types**: BOOLEAN\\n  **Description**: A bool field that indicates whether or not an account was part of Find Test. If 'Null', then the account was not part of a FT. \",\n",
      "      \"metadata\": {\n",
      "        \"documentId\": \"rag-doc\",\n",
      "        \"documentUrl\": \"rag.json\",\n",
      "        \"documentVersion\": \"1.0\",\n",
      "        \"chunkSequence\": 1,\n",
      "        \"chunkTotal\": 4,\n",
      "        \"lastModified\": \"2025-12-29T16:16:30.602554\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"chunkTokens\": \"id: 123\\nname: Test User\\nactive: True\\nscore: 95.5\",\n",
      "      \"metadata\": {\n",
      "        \"documentId\": \"rag-doc\",\n",
      "        \"documentUrl\": \"rag.json\",\n",
      "        \"documentVersion\": \"1.0\",\n",
      "        \"chunkSequence\": 2,\n",
      "        \"chunkTotal\": 4,\n",
      "        \"lastModified\": \"2025-12-29T16:16:30.602588\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"chunkTokens\": \"timestamp: 2024-01-15T10:30:00\\ndata: {\\\"nested\\\": \\\"json\\\", \\\"value\\\": 42}\\ntags: [\\\"tag1\\\", \\\"tag2\\\"]\\nmetadata:\\n  key: value\",\n",
      "      \"metadata\": {\n",
      "        \"documentId\": \"rag-doc\",\n",
      "        \"documentUrl\": \"rag.json\",\n",
      "        \"documentVersion\": \"1.0\",\n",
      "        \"chunkSequence\": 3,\n",
      "        \"chunkTotal\": 4,\n",
      "        \"lastModified\": \"2025-12-29T16:16:30.602611\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"chunkTokens\": \"decimal_val: 123.456\\nbool_str: true\\nnull_str: null\\nempty: \\ncomplex:\\n  ### Header\\n  **Field**: Value\",\n",
      "      \"metadata\": {\n",
      "        \"documentId\": \"rag-doc\",\n",
      "        \"documentUrl\": \"rag.json\",\n",
      "        \"documentVersion\": \"1.0\",\n",
      "        \"chunkSequence\": 4,\n",
      "        \"chunkTotal\": 4,\n",
      "        \"lastModified\": \"2025-12-29T16:16:30.602618\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "\n",
      "================================================================================\n",
      "OPTION 2: JSON Format (Machine Readable)\n",
      "================================================================================\n",
      "{\n",
      "  \"items\": [\n",
      "    {\n",
      "      \"chunkTokens\": \"{\\n  \\\"rule\\\": \\\"fnd test\\\",\\n  \\\"sql\\\": \\\"BAT.FIND_TEST_IND = TRUE\\\",\\n  \\\"category\\\": \\\"FIND_TEST_IND\\\",\\n  \\\"bat_col_meta_info\\\": \\\"### FIND_TEST_IND\\\\n**Attribute Name**: 'FIND_TEST_IND' \\\\n**Data Types**: BOOLEAN\\\\n**Description**: A bool field that indicates whether or not an account was part of Find Test. If 'Null', then the account was not part of a FT. \\\"\\n}\",\n",
      "      \"metadata\": {\n",
      "        \"documentId\": \"rag-doc\",\n",
      "        \"documentUrl\": \"rag.json\",\n",
      "        \"documentVersion\": \"1.0\",\n",
      "        \"chunkSequence\": 1,\n",
      "        \"chunkTotal\": 4,\n",
      "        \"lastModified\": \"2025-12-29T16:16:30.602763\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"chunkTokens\": \"{\\n  \\\"id\\\": 123,\\n  \\\"name\\\": \\\"Test User\\\",\\n  \\\"active\\\": true,\\n  \\\"score\\\": 95.5\\n}\",\n",
      "      \"metadata\": {\n",
      "        \"documentId\": \"rag-doc\",\n",
      "        \"documentUrl\": \"rag.json\",\n",
      "        \"documentVersion\": \"1.0\",\n",
      "        \"chunkSequence\": 2,\n",
      "        \"chunkTotal\": 4,\n",
      "        \"lastModified\": \"2025-12-29T16:16:30.602771\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"chunkTokens\": \"{\\n  \\\"timestamp\\\": \\\"2024-01-15T10:30:00\\\",\\n  \\\"data\\\": \\\"{\\\\\\\"nested\\\\\\\": \\\\\\\"json\\\\\\\", \\\\\\\"value\\\\\\\": 42}\\\",\\n  \\\"tags\\\": [\\n    \\\"tag1\\\",\\n    \\\"tag2\\\"\\n  ],\\n  \\\"metadata\\\": {\\n    \\\"key\\\": \\\"value\\\"\\n  }\\n}\",\n",
      "      \"metadata\": {\n",
      "        \"documentId\": \"rag-doc\",\n",
      "        \"documentUrl\": \"rag.json\",\n",
      "        \"documentVersion\": \"1.0\",\n",
      "        \"chunkSequence\": 3,\n",
      "        \"chunkTotal\": 4,\n",
      "        \"lastModified\": \"2025-12-29T16:16:30.602778\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"chunkTokens\": \"{\\n  \\\"decimal_val\\\": \\\"123.456\\\",\\n  \\\"bool_str\\\": \\\"true\\\",\\n  \\\"null_str\\\": \\\"null\\\",\\n  \\\"empty\\\": \\\"\\\",\\n  \\\"complex\\\": \\\"### Header\\\\n**Field**: Value\\\"\\n}\",\n",
      "      \"metadata\": {\n",
      "        \"documentId\": \"rag-doc\",\n",
      "        \"documentUrl\": \"rag.json\",\n",
      "        \"documentVersion\": \"1.0\",\n",
      "        \"chunkSequence\": 4,\n",
      "        \"chunkTotal\": 4,\n",
      "        \"lastModified\": \"2025-12-29T16:16:30.602785\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "\n",
      "================================================================================\n",
      "CONVERSION TO LANGCHAIN\n",
      "================================================================================\n",
      "Total LangChain Documents: 4\n",
      "\n",
      "Document 1:\n",
      "  Content Preview: {\n",
      "  \"rule\": \"fnd test\",\n",
      "  \"sql\": \"BAT.FIND_TEST_IND = TRUE\",\n",
      "  \"category\": \"FIND_TEST_IND\",\n",
      "  \"bat_c...\n",
      "  Metadata: {'documentId': 'rag-doc', 'documentUrl': 'rag.json', 'documentVersion': '1.0', 'chunkSequence': 1, 'chunkTotal': 4, 'lastModified': '2025-12-29T16:16:30.602763'}\n",
      "\n",
      "Document 2:\n",
      "  Content Preview: {\n",
      "  \"id\": 123,\n",
      "  \"name\": \"Test User\",\n",
      "  \"active\": true,\n",
      "  \"score\": 95.5\n",
      "}...\n",
      "  Metadata: {'documentId': 'rag-doc', 'documentUrl': 'rag.json', 'documentVersion': '1.0', 'chunkSequence': 2, 'chunkTotal': 4, 'lastModified': '2025-12-29T16:16:30.602771'}\n",
      "\n",
      "Document 3:\n",
      "  Content Preview: {\n",
      "  \"timestamp\": \"2024-01-15T10:30:00\",\n",
      "  \"data\": \"{\\\"nested\\\": \\\"json\\\", \\\"value\\\": 42}\",\n",
      "  \"tags\":...\n",
      "  Metadata: {'documentId': 'rag-doc', 'documentUrl': 'rag.json', 'documentVersion': '1.0', 'chunkSequence': 3, 'chunkTotal': 4, 'lastModified': '2025-12-29T16:16:30.602778'}\n",
      "\n",
      "Document 4:\n",
      "  Content Preview: {\n",
      "  \"decimal_val\": \"123.456\",\n",
      "  \"bool_str\": \"true\",\n",
      "  \"null_str\": \"null\",\n",
      "  \"empty\": \"\",\n",
      "  \"complex\"...\n",
      "  Metadata: {'documentId': 'rag-doc', 'documentUrl': 'rag.json', 'documentVersion': '1.0', 'chunkSequence': 4, 'chunkTotal': 4, 'lastModified': '2025-12-29T16:16:30.602785'}\n",
      "\n",
      "================================================================================\n",
      "ACCESSING SPECIFIC CHUNKS\n",
      "================================================================================\n",
      "Chunk 2 Content:\n",
      "{\n",
      "  \"id\": 123,\n",
      "  \"name\": \"Test User\",\n",
      "  \"active\": true,\n",
      "  \"score\": 95.5\n",
      "}\n",
      "\n",
      "Chunk 2 Metadata:\n",
      "  Document ID: rag-doc\n",
      "  Sequence: 2/4\n",
      "  Last Modified: 2025-12-29T16:16:30.602771\n",
      "\n",
      "================================================================================\n",
      "SAVE TO FILE\n",
      "================================================================================\n",
      "✓ Saved to output_rag.json\n",
      "✓ Loaded 4 chunks from file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/41/1w73md7s4dl39qztvt7zf8800000gn/T/ipykernel_98959/2145891994.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  lastModified: str = Field(default_factory=lambda: datetime.utcnow().isoformat())\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Any, Optional, Union\n",
    "from datetime import datetime\n",
    "from langchain_core.documents import Document\n",
    "from pydantic import BaseModel, Field\n",
    "import json\n",
    "\n",
    "\n",
    "class ChunkMetadata(BaseModel):\n",
    "    \"\"\"Metadata for each document chunk\"\"\"\n",
    "    documentId: str\n",
    "    documentUrl: str\n",
    "    documentVersion: str\n",
    "    chunkSequence: int\n",
    "    chunkTotal: int\n",
    "    lastModified: str = Field(default_factory=lambda: datetime.utcnow().isoformat())\n",
    "\n",
    "\n",
    "class CustomChunk(BaseModel):\n",
    "    \"\"\"Individual chunk with content and metadata\"\"\"\n",
    "    chunkTokens: str\n",
    "    metadata: ChunkMetadata\n",
    "    \n",
    "    def to_langchain(self) -> Document:\n",
    "        \"\"\"Convert to LangChain Document\"\"\"\n",
    "        return Document(\n",
    "            page_content=self.chunkTokens,\n",
    "            metadata=self.metadata.model_dump()\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def from_langchain(cls, doc: Document) -> 'CustomChunk':\n",
    "        \"\"\"Create from LangChain Document\"\"\"\n",
    "        return cls(\n",
    "            chunkTokens=doc.page_content,\n",
    "            metadata=ChunkMetadata(**doc.metadata)\n",
    "        )\n",
    "\n",
    "\n",
    "class CustomDocumentCollection(BaseModel):\n",
    "    \"\"\"Collection of document chunks\"\"\"\n",
    "    items: List[CustomChunk]\n",
    "    \n",
    "    def to_json(self, filepath: Optional[str] = None, **kwargs) -> str:\n",
    "        \"\"\"Export to JSON format\"\"\"\n",
    "        json_str = self.model_dump_json(indent=2, **kwargs)\n",
    "        if filepath:\n",
    "            with open(filepath, 'w') as f:\n",
    "                f.write(json_str)\n",
    "        return json_str\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Export to dictionary format\"\"\"\n",
    "        return self.model_dump()\n",
    "    \n",
    "    @classmethod\n",
    "    def from_json(cls, json_data: str) -> 'CustomDocumentCollection':\n",
    "        \"\"\"Load from JSON string or file\"\"\"\n",
    "        if json_data.endswith('.json'):\n",
    "            with open(json_data, 'r') as f:\n",
    "                json_data = f.read()\n",
    "        return cls.model_validate_json(json_data)\n",
    "    \n",
    "    def to_langchain_docs(self) -> List[Document]:\n",
    "        \"\"\"Convert all chunks to LangChain Documents\"\"\"\n",
    "        return [chunk.to_langchain() for chunk in self.items]\n",
    "    \n",
    "    @classmethod\n",
    "    def from_langchain_docs(cls, docs: List[Document], \n",
    "                           document_id: str,\n",
    "                           document_url: str,\n",
    "                           document_version: str = \"1.0\") -> 'CustomDocumentCollection':\n",
    "        \"\"\"Create from list of LangChain Documents\"\"\"\n",
    "        total_chunks = len(docs)\n",
    "        chunks = []\n",
    "        \n",
    "        for idx, doc in enumerate(docs, start=1):\n",
    "            metadata = ChunkMetadata(\n",
    "                documentId=document_id,\n",
    "                documentUrl=document_url,\n",
    "                documentVersion=document_version,\n",
    "                chunkSequence=idx,\n",
    "                chunkTotal=total_chunks\n",
    "            )\n",
    "            chunks.append(CustomChunk(\n",
    "                chunkTokens=doc.page_content,\n",
    "                metadata=metadata\n",
    "            ))\n",
    "        \n",
    "        return cls(items=chunks)\n",
    "\n",
    "\n",
    "def dict_to_text(data: Dict[str, Any], indent: int = 0) -> str:\n",
    "    \"\"\"Convert dictionary to formatted text representation\"\"\"\n",
    "    lines = []\n",
    "    prefix = \"  \" * indent\n",
    "    \n",
    "    for key, value in data.items():\n",
    "        if isinstance(value, dict):\n",
    "            lines.append(f\"{prefix}{key}:\")\n",
    "            lines.append(dict_to_text(value, indent + 1))\n",
    "        elif isinstance(value, list):\n",
    "            lines.append(f\"{prefix}{key}: {json.dumps(value)}\")\n",
    "        elif isinstance(value, str) and ('\\n' in value or len(value) > 100):\n",
    "            # Handle multiline strings\n",
    "            lines.append(f\"{prefix}{key}:\")\n",
    "            for line in value.split('\\n'):\n",
    "                lines.append(f\"{prefix}  {line}\")\n",
    "        else:\n",
    "            lines.append(f\"{prefix}{key}: {value}\")\n",
    "    \n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "\n",
    "def create_document_from_list(\n",
    "    data_list: List[Dict[str, Any]],\n",
    "    document_id: str,\n",
    "    document_url: str,\n",
    "    document_version: str = \"1.0\",\n",
    "    chunk_by: str = \"item\"  # \"item\" or \"size\"\n",
    ") -> CustomDocumentCollection:\n",
    "    \"\"\"\n",
    "    Create document collection from a list of dictionaries\n",
    "    \n",
    "    Args:\n",
    "        data_list: List of dictionaries to convert\n",
    "        document_id: Document identifier\n",
    "        document_url: Document URL/path\n",
    "        document_version: Version string\n",
    "        chunk_by: \"item\" (one chunk per item) or \"size\" (split by character count)\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    \n",
    "    if chunk_by == \"item\":\n",
    "        # Create one chunk per list item\n",
    "        total_chunks = len(data_list)\n",
    "        \n",
    "        for idx, item in enumerate(data_list, start=1):\n",
    "            # Convert dict to readable text format\n",
    "            chunk_text = dict_to_text(item)\n",
    "            \n",
    "            metadata = ChunkMetadata(\n",
    "                documentId=document_id,\n",
    "                documentUrl=document_url,\n",
    "                documentVersion=document_version,\n",
    "                chunkSequence=idx,\n",
    "                chunkTotal=total_chunks\n",
    "            )\n",
    "            \n",
    "            chunks.append(CustomChunk(\n",
    "                chunkTokens=chunk_text,\n",
    "                metadata=metadata\n",
    "            ))\n",
    "    \n",
    "    return CustomDocumentCollection(items=chunks)\n",
    "\n",
    "\n",
    "def create_document_from_list_json(\n",
    "    data_list: List[Dict[str, Any]],\n",
    "    document_id: str,\n",
    "    document_url: str,\n",
    "    document_version: str = \"1.0\",\n",
    "    preserve_json: bool = True\n",
    ") -> CustomDocumentCollection:\n",
    "    \"\"\"\n",
    "    Create document collection preserving JSON structure\n",
    "    \n",
    "    Args:\n",
    "        data_list: List of dictionaries to convert\n",
    "        document_id: Document identifier\n",
    "        document_url: Document URL/path\n",
    "        document_version: Version string\n",
    "        preserve_json: If True, store as JSON string; if False, use formatted text\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    total_chunks = len(data_list)\n",
    "    \n",
    "    for idx, item in enumerate(data_list, start=1):\n",
    "        if preserve_json:\n",
    "            # Store as JSON string\n",
    "            chunk_text = json.dumps(item, indent=2, ensure_ascii=False)\n",
    "        else:\n",
    "            # Store as formatted text\n",
    "            chunk_text = dict_to_text(item)\n",
    "        \n",
    "        metadata = ChunkMetadata(\n",
    "            documentId=document_id,\n",
    "            documentUrl=document_url,\n",
    "            documentVersion=document_version,\n",
    "            chunkSequence=idx,\n",
    "            chunkTotal=total_chunks\n",
    "        )\n",
    "        \n",
    "        chunks.append(CustomChunk(\n",
    "            chunkTokens=chunk_text,\n",
    "            metadata=metadata\n",
    "        ))\n",
    "    \n",
    "    return CustomDocumentCollection(items=chunks)\n",
    "\n",
    "\n",
    "# Test with your sample data\n",
    "if __name__ == \"__main__\":\n",
    "    data_list = [\n",
    "        {\n",
    "            \"rule\": \"fnd test\",\n",
    "            \"sql\": \"BAT.FIND_TEST_IND = TRUE\",\n",
    "            \"category\": \"FIND_TEST_IND\",\n",
    "            \"bat_col_meta_info\": \"### FIND_TEST_IND\\n**Attribute Name**: 'FIND_TEST_IND' \\n**Data Types**: BOOLEAN\\n**Description**: A bool field that indicates whether or not an account was part of Find Test. If 'Null', then the account was not part of a FT. \"\n",
    "        },\n",
    "        {\n",
    "            \"id\": 123,\n",
    "            \"name\": \"Test User\",\n",
    "            \"active\": True,\n",
    "            \"score\": 95.5\n",
    "        },\n",
    "        {\n",
    "            \"timestamp\": \"2024-01-15T10:30:00\",\n",
    "            \"data\": '{\"nested\": \"json\", \"value\": 42}',\n",
    "            \"tags\": [\"tag1\", \"tag2\"],\n",
    "            \"metadata\": {\"key\": \"value\"}\n",
    "        },\n",
    "        {\n",
    "            \"decimal_val\": \"123.456\",\n",
    "            \"bool_str\": \"true\",\n",
    "            \"null_str\": \"null\",\n",
    "            \"empty\": \"\",\n",
    "            \"complex\": \"### Header\\n**Field**: Value\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"OPTION 1: Formatted Text (Human Readable)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Create document collection with formatted text\n",
    "    doc_collection_text = create_document_from_list(\n",
    "        data_list=data_list,\n",
    "        document_id=\"rag-doc\",\n",
    "        document_url=\"rag.json\",\n",
    "        document_version=\"1.0\"\n",
    "    )\n",
    "    \n",
    "    print(doc_collection_text.to_json())\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"OPTION 2: JSON Format (Machine Readable)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Create document collection preserving JSON\n",
    "    doc_collection_json = create_document_from_list_json(\n",
    "        data_list=data_list,\n",
    "        document_id=\"rag-doc\",\n",
    "        document_url=\"rag.json\",\n",
    "        document_version=\"1.0\",\n",
    "        preserve_json=True\n",
    "    )\n",
    "    \n",
    "    print(doc_collection_json.to_json())\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"CONVERSION TO LANGCHAIN\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Convert to LangChain documents\n",
    "    langchain_docs = doc_collection_json.to_langchain_docs()\n",
    "    \n",
    "    print(f\"Total LangChain Documents: {len(langchain_docs)}\\n\")\n",
    "    \n",
    "    for i, doc in enumerate(langchain_docs, 1):\n",
    "        print(f\"Document {i}:\")\n",
    "        print(f\"  Content Preview: {doc.page_content[:100]}...\")\n",
    "        print(f\"  Metadata: {doc.metadata}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"ACCESSING SPECIFIC CHUNKS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Access specific chunk\n",
    "    chunk_2 = doc_collection_json.items[1]\n",
    "    print(f\"Chunk 2 Content:\\n{chunk_2.chunkTokens}\\n\")\n",
    "    print(f\"Chunk 2 Metadata:\")\n",
    "    print(f\"  Document ID: {chunk_2.metadata.documentId}\")\n",
    "    print(f\"  Sequence: {chunk_2.metadata.chunkSequence}/{chunk_2.metadata.chunkTotal}\")\n",
    "    print(f\"  Last Modified: {chunk_2.metadata.lastModified}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SAVE TO FILE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Save to file\n",
    "    doc_collection_json.to_json(\"output_rag.json\")\n",
    "    print(\"✓ Saved to output_rag.json\")\n",
    "    \n",
    "    # Load from file\n",
    "    loaded = CustomDocumentCollection.from_json(\"output_rag.json\")\n",
    "    print(f\"✓ Loaded {len(loaded.items)} chunks from file\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
